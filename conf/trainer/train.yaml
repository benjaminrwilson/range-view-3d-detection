##########################################################################################################
# Trainer Configuration                                                                                  #
##########################################################################################################

_target_: pytorch_lightning.trainer.trainer.Trainer
accelerator: auto # torch.distributed backend. Defaults to distributed data parallel.
benchmark: true
# default_root_dir: ${hydra:runtime.output_dir}
default_root_dir: ${hydra:runtime.output_dir}
detect_anomaly: false
devices: auto # Number of GPUs.
gradient_clip_val: 35.0 # Maximum gradient norm before clipping.
max_epochs: 20 # Maximum number of epochs during training.
precision: bf16-mixed
sync_batchnorm: true
check_val_every_n_epoch: ${.max_epochs}

callbacks:
  - _target_: pytorch_lightning.callbacks.LearningRateMonitor
  # - _target_: pytorch_lightning.callbacks.ModelCheckpoint
  #   dirpath: ${hydra:runtime.output_dir}/checkpoints
  #   # every_n_epochs: 1
  #   # save_top_k: -1
  #   save_on_train_epoch_end: true

# logger:
#   _target_: pytorch_lightning.loggers.TensorBoardLogger
#   save_dir: ${hydra:runtime.output_dir}
#   default_hp_metric: true

logger:
  _target_: pytorch_lightning.loggers.WandbLogger
  save_dir: ${..default_root_dir}
  dir: ${..default_root_dir}
  log_model: true
  group: ${name}
  project: ${dataset.dataset_name}-release

strategy:
  _target_: pytorch_lightning.strategies.DDPStrategy
  find_unused_parameters: false
  gradient_as_bucket_view: true
  static_graph: true
  process_group_backend: nccl
